{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://www.nengo.ai/nengo-examples/loihi/cifar10-convnet.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m urlretrieve\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnengo\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnengo_dl\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import functools\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nengo\n",
    "import nengo_dl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import nengo_loihi\n",
    "\n",
    "\n",
    "def partial(func, *args, **kwargs):\n",
    "    \"\"\"Helper to call functools.partial and copy over func.__name__\"\"\"\n",
    "    new_func = functools.partial(func, *args, **kwargs)\n",
    "    functools.update_wrapper(new_func, func)\n",
    "    return new_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_l2_loss_range(\n",
    "    y_true, y, sample_weight=None, min_rate=0.0, max_rate=np.inf, percentile=99.0\n",
    "):\n",
    "    # y axes are (batch examples, time (==1), neurons)\n",
    "    assert len(y.shape) == 3\n",
    "    rates = tfp.stats.percentile(y, percentile, axis=(0, 1))\n",
    "    low_error = tf.maximum(0.0, min_rate - rates)\n",
    "    high_error = tf.maximum(0.0, rates - max_rate)\n",
    "    loss = tf.nn.l2_loss(low_error + high_error)\n",
    "\n",
    "    return (sample_weight * loss) if sample_weight is not None else loss\n",
    "\n",
    "\n",
    "def slice_data_dict(data, slice_):\n",
    "    return {key: value[slice_] for key, value in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NengoImageIterator(tf.keras.preprocessing.image.Iterator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_data_generator,\n",
    "        x_keys,\n",
    "        x,\n",
    "        y_keys,\n",
    "        y,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        sample_weight=None,\n",
    "        seed=None,\n",
    "        subset=None,\n",
    "        dtype=\"float32\",\n",
    "    ):\n",
    "        assert subset is None, \"Not Implemented\"\n",
    "        assert isinstance(x_keys, (tuple, list))\n",
    "        assert isinstance(y_keys, (tuple, list))\n",
    "        assert isinstance(x, (tuple, list))\n",
    "        assert isinstance(y, (tuple, list))\n",
    "\n",
    "        self.dtype = dtype\n",
    "        self.x_keys = x_keys\n",
    "        self.y_keys = y_keys\n",
    "\n",
    "        x0 = x[0]\n",
    "        assert all(len(xx) == len(x0) for xx in x), (\n",
    "            \"All of the arrays in `x` should have the same length. \"\n",
    "            \"[len(xx) for xx in x] = %s\" % ([len(xx) for xx in x],)\n",
    "        )\n",
    "        assert all(len(yy) == len(x0) for yy in y), (\n",
    "            \"All of the arrays in `y` should have the same length as `x`. \"\n",
    "            \"len(x[0]) = %d, [len(yy) for yy in y] = %s\"\n",
    "            % (len(x0), [len(yy) for yy in y])\n",
    "        )\n",
    "        assert len(x_keys) == len(x)\n",
    "        assert len(y_keys) == len(y)\n",
    "\n",
    "        if sample_weight is not None and len(x0) != len(sample_weight):\n",
    "            raise ValueError(\n",
    "                \"`x[0]` (images tensor) and `sample_weight` \"\n",
    "                \"should have the same length. \"\n",
    "                \"Found: x.shape = %s, sample_weight.shape = %s\"\n",
    "                % (np.asarray(x0).shape, np.asarray(sample_weight).shape)\n",
    "            )\n",
    "\n",
    "        self.x = [\n",
    "            np.asarray(xx, dtype=self.dtype if i == 0 else None)\n",
    "            for i, xx in enumerate(x)\n",
    "        ]\n",
    "        if self.x[0].ndim != 4:\n",
    "            raise ValueError(\n",
    "                \"Input data in `NumpyArrayIterator` \"\n",
    "                \"should have rank 4. You passed an array \"\n",
    "                \"with shape\",\n",
    "                self.x[0].shape,\n",
    "            )\n",
    "\n",
    "        self.y = [np.asarray(yy) for yy in y]\n",
    "        self.sample_weight = (\n",
    "            None if sample_weight is None else np.asarray(sample_weight)\n",
    "        )\n",
    "        self.image_data_generator = image_data_generator\n",
    "        super().__init__(self.x[0].shape[0], batch_size, shuffle, seed)\n",
    "\n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        images = self.x[0]\n",
    "        assert images.dtype == self.dtype\n",
    "\n",
    "        n = len(index_array)\n",
    "        batch_x = np.zeros((n,) + images[0].shape, dtype=self.dtype)\n",
    "        for i, j in enumerate(index_array):\n",
    "            x = images[j]\n",
    "            params = self.image_data_generator.get_random_transform(x.shape)\n",
    "            x = self.image_data_generator.apply_transform(x, params)\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "            batch_x[i] = x\n",
    "\n",
    "        batch_x_miscs = [xx[index_array] for xx in self.x[1:]]\n",
    "        batch_y_miscs = [yy[index_array] for yy in self.y]\n",
    "\n",
    "        x_pairs = [\n",
    "            (k, self.x_postprocess(k, v))\n",
    "            for k, v in zip(self.x_keys, [batch_x] + batch_x_miscs)\n",
    "        ]\n",
    "        y_pairs = [\n",
    "            (k, self.y_postprocess(k, v)) for k, v in zip(self.y_keys, batch_y_miscs)\n",
    "        ]\n",
    "\n",
    "        output = (\n",
    "            collections.OrderedDict(x_pairs),\n",
    "            collections.OrderedDict(y_pairs),\n",
    "        )\n",
    "\n",
    "        if self.sample_weight is not None:\n",
    "            output += (self.sample_weight[index_array],)\n",
    "        return output\n",
    "\n",
    "    def x_postprocess(self, key, x):\n",
    "        return x if key == \"n_steps\" else x.reshape((x.shape[0], 1, -1))\n",
    "\n",
    "    def y_postprocess(self, key, y):\n",
    "        return y.reshape((y.shape[0], 1, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_last = True\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "n_classes = len(np.unique(train_y))\n",
    "\n",
    "# TensorFlow does not include the label names, so define them manually\n",
    "label_names = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "assert n_classes == len(label_names)\n",
    "\n",
    "if not channels_last:\n",
    "    train_x = np.transpose(train_x, (0, 3, 1, 2))\n",
    "    test_x = np.transpose(test_x, (0, 3, 1, 2))\n",
    "\n",
    "# convert the images to float32, and rescale to [-1, 1]\n",
    "train_x = train_x.astype(np.float32) / 127.5 - 1\n",
    "test_x = test_x.astype(np.float32) / 127.5 - 1\n",
    "\n",
    "train_t = np.array(\n",
    "    tf.one_hot(train_y, n_classes, on_value=1, off_value=0), dtype=np.float32\n",
    ")\n",
    "test_t = np.array(\n",
    "    tf.one_hot(test_y, n_classes, on_value=1, off_value=0), dtype=np.float32\n",
    ")\n",
    "\n",
    "train_y = train_y.squeeze()\n",
    "test_y = test_y.squeeze()\n",
    "\n",
    "train_x_flat = train_x.reshape((train_x.shape[0], 1, -1))\n",
    "train_t_flat = train_t.reshape((train_t.shape[0], 1, -1))\n",
    "\n",
    "test_x_flat = test_x.reshape((test_x.shape[0], 1, -1))\n",
    "test_t_flat = test_t.reshape((test_t.shape[0], 1, -1))\n",
    "\n",
    "input_shape = nengo.transforms.ChannelShape(\n",
    "    test_x[0].shape, channels_last=channels_last\n",
    ")\n",
    "assert input_shape.n_channels in (1, 3)\n",
    "assert train_x[0].shape == test_x[0].shape == input_shape.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rate = 150\n",
    "amp = 1.0 / max_rate\n",
    "rate_reg = 1e-3\n",
    "rate_target = max_rate * amp  # must be in amplitude scaled units\n",
    "\n",
    "relu = nengo.SpikingRectifiedLinear(amplitude=amp)\n",
    "chip_neuron = nengo_loihi.neurons.LoihiLIF(amplitude=amp)\n",
    "\n",
    "layer_confs = [\n",
    "    dict(\n",
    "        name=\"input-layer\",\n",
    "        filters=4,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        neuron=relu,\n",
    "        on_chip=False,\n",
    "    ),\n",
    "    dict(name=\"conv-layer1\", filters=64, kernel_size=3, strides=2, block=(8, 8, 16)),\n",
    "    dict(name=\"conv-layer2\", filters=72, kernel_size=3, strides=1, block=(7, 7, 8)),\n",
    "    dict(name=\"conv-layer3\", filters=256, kernel_size=3, strides=2, block=(6, 6, 12)),\n",
    "    dict(name=\"conv-layer4\", filters=256, kernel_size=1, strides=1, block=(6, 6, 24)),\n",
    "    dict(name=\"conv-layer5\", filters=64, kernel_size=1, strides=1, block=(6, 6, 24)),\n",
    "    dict(name=\"dense-layer\", n_neurons=100, block=(50,)),\n",
    "    dict(name=\"output-layer\", n_neurons=10, neuron=None, on_chip=False),\n",
    "]\n",
    "\n",
    "# Create a PresentInput process to show images from the training set sequentially.\n",
    "# Each image is presented for `presentation_time` seconds.\n",
    "# NOTE: this is not used during training, since we get `nengo_dl` to override the\n",
    "# output of this node with the training data.\n",
    "presentation_time = 0.2\n",
    "present_images = nengo.processes.PresentInput(test_x_flat, presentation_time)\n",
    "\n",
    "total_n_neurons = 0\n",
    "total_n_weights = 0\n",
    "\n",
    "with nengo.Network() as net:\n",
    "    net.config[nengo.Ensemble].max_rates = nengo.dists.Choice([max_rate])\n",
    "    net.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "\n",
    "    # we set the learning phase to True (training) to always use rate neurons\n",
    "    nengo_dl.configure_settings(learning_phase=True)\n",
    "\n",
    "    # add a configurable keep_history option to Probes (we'll set this\n",
    "    # to False for some probes below)\n",
    "    nengo_dl.configure_settings(keep_history=True)\n",
    "\n",
    "    # this is an optimization to improve the training speed,\n",
    "    # since we won't require stateful behaviour in this example\n",
    "    nengo_dl.configure_settings(stateful=False)\n",
    "\n",
    "    # this sets the amount of smoothing used on the LIF neurons during training\n",
    "    nengo_dl.configure_settings(lif_smoothing=0.01)\n",
    "\n",
    "    # this allows us to set `nengo_loihi` parameters like `on_chip` and `block_shape`\n",
    "    nengo_loihi.add_params(net)\n",
    "\n",
    "    # the input node that will be used to feed in input images\n",
    "    inp = nengo.Node(present_images, label=\"input_node\")\n",
    "\n",
    "    connections = []\n",
    "    transforms = []\n",
    "    layer_probes = []\n",
    "    shape_in = input_shape\n",
    "    x = inp\n",
    "    for k, layer_conf in enumerate(layer_confs):\n",
    "        layer_conf = dict(layer_conf)  # copy, so we don't modify the original\n",
    "        name = layer_conf.pop(\"name\")\n",
    "        neuron_type = layer_conf.pop(\"neuron\", chip_neuron)\n",
    "        on_chip = layer_conf.pop(\"on_chip\", True)\n",
    "        block = layer_conf.pop(\"block\", None)\n",
    "\n",
    "        if block is not None and not channels_last:\n",
    "            # move channels to first index\n",
    "            block = (block[-1],) + block[:-1]\n",
    "\n",
    "        # --- create layer transform\n",
    "        if \"filters\" in layer_conf:\n",
    "            # convolutional layer\n",
    "            n_filters = layer_conf.pop(\"filters\")\n",
    "            kernel_size = layer_conf.pop(\"kernel_size\")\n",
    "            strides = layer_conf.pop(\"strides\", 1)\n",
    "            assert len(layer_conf) == 0, \"Unused fields in conv layer: %s\" % list(\n",
    "                layer_conf\n",
    "            )\n",
    "\n",
    "            kernel_size = (\n",
    "                (kernel_size, kernel_size)\n",
    "                if isinstance(kernel_size, int)\n",
    "                else kernel_size\n",
    "            )\n",
    "            strides = (strides, strides) if isinstance(strides, int) else strides\n",
    "\n",
    "            transform = nengo.Convolution(\n",
    "                n_filters=n_filters,\n",
    "                input_shape=shape_in,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=strides,\n",
    "                padding=\"valid\",\n",
    "                channels_last=channels_last,\n",
    "                init=nengo_dl.dists.Glorot(scale=1.0 / np.prod(kernel_size)),\n",
    "            )\n",
    "            shape_out = transform.output_shape\n",
    "\n",
    "            loc = \"chip\" if on_chip else \"host\"\n",
    "            n_neurons = np.prod(shape_out.shape)\n",
    "            n_weights = np.prod(transform.kernel_shape)\n",
    "            print(\n",
    "                \"%s: %s: conv %s, stride %s, output %s (%d neurons, %d weights)\"\n",
    "                % (\n",
    "                    loc,\n",
    "                    name,\n",
    "                    kernel_size,\n",
    "                    strides,\n",
    "                    shape_out.shape,\n",
    "                    n_neurons,\n",
    "                    n_weights,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # dense layer\n",
    "            n_neurons = layer_conf.pop(\"n_neurons\")\n",
    "\n",
    "            shape_out = nengo.transforms.ChannelShape((n_neurons,))\n",
    "            transform = nengo.Dense(\n",
    "                (shape_out.size, shape_in.size),\n",
    "                init=nengo_dl.dists.Glorot(),\n",
    "            )\n",
    "\n",
    "            loc = \"chip\" if on_chip else \"host\"\n",
    "            n_weights = np.prod(transform.shape)\n",
    "            print(\n",
    "                \"%s: %s: dense %d, output %s (%d neurons, %d weights)\"\n",
    "                % (loc, name, n_neurons, shape_out.shape, n_neurons, n_weights)\n",
    "            )\n",
    "\n",
    "        assert len(layer_conf) == 0, \"Unused fields in %s: %s\" % (\n",
    "            [name] + list(layer_conf)\n",
    "        )\n",
    "\n",
    "        total_n_neurons += n_neurons\n",
    "        total_n_weights += n_weights\n",
    "\n",
    "        # --- create layer output (Ensemble or Node)\n",
    "        assert on_chip or block is None, \"`block` must be None if off-chip\"\n",
    "\n",
    "        if neuron_type is None:\n",
    "            assert not on_chip, \"Nodes can only be run off-chip\"\n",
    "            y = nengo.Node(size_in=shape_out.size, label=name)\n",
    "        else:\n",
    "            ens = nengo.Ensemble(shape_out.size, 1, neuron_type=neuron_type, label=name)\n",
    "            net.config[ens].on_chip = on_chip\n",
    "            y = ens.neurons\n",
    "\n",
    "            if block is not None:\n",
    "                net.config[ens].block_shape = nengo_loihi.BlockShape(\n",
    "                    block,\n",
    "                    shape_out.shape,\n",
    "                )\n",
    "\n",
    "            # add a probe so we can measure individual layer rates\n",
    "            probe = nengo.Probe(y, synapse=None, label=\"%s_p\" % name)\n",
    "            net.config[probe].keep_history = False\n",
    "            layer_probes.append(probe)\n",
    "\n",
    "        conn = nengo.Connection(x, y, transform=transform)\n",
    "        net.config[conn].pop_type = 32\n",
    "\n",
    "        transforms.append(transform)\n",
    "        connections.append(conn)\n",
    "        x = y\n",
    "        shape_in = shape_out\n",
    "\n",
    "    output_p = nengo.Probe(x, synapse=None, label=\"output_p\")\n",
    "\n",
    "print(\"TOTAL: %d neurons, %d weights\" % (total_n_neurons, total_n_weights))\n",
    "assert len(layer_confs) == len(transforms) == len(connections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input and target dictionaries to pass to NengoDL\n",
    "train_inputs = {inp: train_x_flat}\n",
    "train_targets = {output_p: train_t_flat}\n",
    "\n",
    "test_inputs = {inp: test_x_flat}\n",
    "test_targets = {output_p: test_t_flat}\n",
    "for probe in layer_probes:\n",
    "    train_targets[probe] = np.zeros((train_t_flat.shape[0], 1, 0), dtype=np.float32)\n",
    "    test_targets[probe] = np.zeros((test_t_flat.shape[0], 1, 0), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- evaluate layers\n",
    "with nengo_dl.Simulator(net, minibatch_size=100, progress_bar=False) as sim:\n",
    "    for conf, conn in zip(layer_confs, connections):\n",
    "        weights = sim.model.sig[conn][\"weights\"].initial_value\n",
    "        print(\"%s: initial weights: %0.3f\" % (conf[\"name\"], np.abs(weights).mean()))\n",
    "\n",
    "    sim.run_steps(1, data={inp: train_x_flat[:100]})\n",
    "\n",
    "for conf, layer_probe in zip(layer_confs, layer_probes):\n",
    "    out = sim.data[layer_probe][-1]\n",
    "    print(\"%s: initial rates: %0.3f\" % (conf[\"name\"], np.mean(out)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = True\n",
    "\n",
    "checkpoint_base = \"./cifar10_convnet_params\"\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_idg = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    rotation_range=20,\n",
    "    horizontal_flip=True,\n",
    "    data_format=\"channels_last\" if channels_last else \"channels_first\",\n",
    ")\n",
    "train_idg.fit(train_x)\n",
    "\n",
    "with nengo_dl.Simulator(net, minibatch_size=batch_size) as sim:\n",
    "    percentile = 99.9\n",
    "\n",
    "    def rate_metric(_, outputs):\n",
    "        # take percentile over all examples, for each neuron\n",
    "        top_rates = tfp.stats.percentile(outputs, percentile, axis=(0, 1))\n",
    "        return tf.reduce_mean(top_rates) / amp\n",
    "\n",
    "    losses = collections.OrderedDict()\n",
    "    metrics = collections.OrderedDict()\n",
    "    loss_weights = collections.OrderedDict()\n",
    "\n",
    "    losses[output_p] = tf.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    metrics[output_p] = \"accuracy\"\n",
    "    loss_weights[output_p] = 1.0\n",
    "\n",
    "    for probe, layer_conf in zip(layer_probes, layer_confs):\n",
    "        metrics[probe] = rate_metric\n",
    "\n",
    "        if layer_conf.get(\"on_chip\", True):\n",
    "            losses[probe] = partial(\n",
    "                percentile_l2_loss_range,\n",
    "                min_rate=0.5 * rate_target,\n",
    "                max_rate=rate_target,\n",
    "                percentile=percentile,\n",
    "            )\n",
    "            loss_weights[probe] = rate_reg\n",
    "        else:\n",
    "            losses[probe] = partial(\n",
    "                percentile_l2_loss_range,\n",
    "                min_rate=0,\n",
    "                max_rate=rate_target,\n",
    "                percentile=percentile,\n",
    "            )\n",
    "            loss_weights[probe] = 10 * rate_reg\n",
    "\n",
    "    sim.compile(\n",
    "        loss=losses,\n",
    "        optimizer=tf.optimizers.Adam(),\n",
    "        metrics=metrics,\n",
    "        loss_weights=loss_weights,\n",
    "    )\n",
    "\n",
    "    if do_training:\n",
    "        # --- train\n",
    "        steps_per_epoch = len(train_x) // batch_size\n",
    "\n",
    "        # Create a NengoImageIterator that will return the appropriate dictionaries\n",
    "        # with augmented images. Since we are using a generator, we need to include\n",
    "        # the `n_steps` parameter so that NengoDL knows how many timesteps are in\n",
    "        # each example (in our case, since we just have static images, it's one).\n",
    "        n = steps_per_epoch * batch_size\n",
    "        n_steps = np.ones((n, 1), dtype=np.int32)\n",
    "        train_data = NengoImageIterator(\n",
    "            image_data_generator=train_idg,\n",
    "            x_keys=[inp.label, \"n_steps\"],\n",
    "            x=[train_x[:n], n_steps],\n",
    "            y_keys=[output_p.label] + [probe.label for probe in layer_probes],\n",
    "            y=[train_t[:n]]\n",
    "            + [np.zeros((n, 1, 0), dtype=np.float32) for _ in layer_probes],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        n_epochs = 100\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            sim.fit(\n",
    "                train_data,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                epochs=1,\n",
    "                verbose=2,\n",
    "            )\n",
    "\n",
    "            # report test data statistics\n",
    "            outputs = sim.evaluate(x=test_inputs, y=test_targets, verbose=0)\n",
    "            print(\"Epoch %d test: %s\" % (epoch, outputs))\n",
    "\n",
    "            # save the parameters to the checkpoint\n",
    "            savefile = checkpoint_base\n",
    "            sim.save_params(savefile)\n",
    "            print(\"Saved params to %r\" % savefile)\n",
    "    else:\n",
    "        urlretrieve(\n",
    "            \"https://drive.google.com/uc?export=download&\"\n",
    "            \"id=1jvP8IsdqGH2kn0OJOykJxjBsJLgk8GsY\",\n",
    "            \"%s.npz\" % checkpoint_base,\n",
    "        )\n",
    "        sim.load_params(checkpoint_base)\n",
    "        print(\"Loaded params %r\" % checkpoint_base)\n",
    "\n",
    "    # copy the learned/loaded parameters back to the network, for Loihi simulator\n",
    "    sim.freeze_params(net)\n",
    "\n",
    "    # run the network on some of the train and test data to benchmark performance\n",
    "    try:\n",
    "        train_slice = slice(0, 1000)\n",
    "        train_outputs = sim.evaluate(\n",
    "            x=slice_data_dict(train_inputs, train_slice),\n",
    "            y=slice_data_dict(train_targets, train_slice),\n",
    "            verbose=0,\n",
    "        )\n",
    "        print(\"Final train:\")\n",
    "        for key, val in train_outputs.items():\n",
    "            print(\"  %s: %s\" % (key, val))\n",
    "\n",
    "        # test_slice = slice(None)\n",
    "        test_slice = slice(0, 1000)\n",
    "        test_outputs = sim.evaluate(\n",
    "            x=slice_data_dict(test_inputs, test_slice),\n",
    "            y=slice_data_dict(test_targets, test_slice),\n",
    "            verbose=0,\n",
    "        )\n",
    "        print(\"Final test:\")\n",
    "        for key, val in test_outputs.items():\n",
    "            print(\"  %s: %s\" % (key, val))\n",
    "    except Exception as e:\n",
    "        print(\"Could not compute ANN values on this machine: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove layer probes\n",
    "for probe in layer_probes:\n",
    "    if probe in net.probes:\n",
    "        net.probes.remove(probe)\n",
    "\n",
    "# add synapses to connections\n",
    "for conn in net.all_connections:\n",
    "    conn.synapse = nengo.synapses.Lowpass(0.01)\n",
    "\n",
    "n_images = 10\n",
    "\n",
    "sim_time = n_images * presentation_time\n",
    "\n",
    "with nengo_loihi.Simulator(net) as sim:\n",
    "    # print information about how cores are being utilized\n",
    "    print(\"\\n\".join(sim.model.utilization_summary()))\n",
    "\n",
    "    sim.run(sim_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_steps = int(presentation_time / sim.dt)\n",
    "class_steps = int(0.3 * pres_steps)\n",
    "\n",
    "output = sim.data[output_p]\n",
    "output = output.reshape((n_images, pres_steps) + output[0].shape)\n",
    "output = output[:, -class_steps:].mean(axis=1)\n",
    "preds = np.argmax(output, axis=-1)\n",
    "\n",
    "assert preds.shape == test_y[:n_images].shape\n",
    "\n",
    "print(\"Predictions: %s\" % (list(preds),))\n",
    "print(\"Actual:      %s\" % (list(test_y[:n_images]),))\n",
    "error = (preds != test_y[:n_images]).mean()\n",
    "print(\"Accuracy: %0.3f%%, Error: %0.3f%%\" % (100 * (1 - error), 100 * error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "images = test_x if channels_last else np.transpose(test_x, (0, 2, 3, 1))\n",
    "ni, nj, nc = images[0].shape\n",
    "allimage = np.zeros((ni, nj * n_images, nc))\n",
    "for i, image in enumerate(images[:n_images]):\n",
    "    allimage[:, i * nj : (i + 1) * nj] = image\n",
    "if allimage.shape[-1] == 1:\n",
    "    allimage = allimage[:, :, 0]\n",
    "allimage = (allimage + 1) / 2  # scale to [0, 1]\n",
    "plt.imshow(allimage, aspect=\"auto\", interpolation=\"none\", cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "t = sim.trange()\n",
    "plt.plot(t, sim.data[output_p])\n",
    "plt.xlim([t[0], t[-1]])\n",
    "plt.xlabel(\"time [s]\")\n",
    "plt.legend(label_names, loc=\"upper right\", bbox_to_anchor=(1.18, 1.05))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike-mnn-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
